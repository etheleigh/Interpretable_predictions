{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrY1KXAu7xbu"
   },
   "source": [
    "# Recommender System for Board Games from [BoardGameGeek.com](https://boardgamegeek.com/)\n",
    "\n",
    "\n",
    "\n",
    "1.   Iatrou Manos\n",
    "2.   Papageorgiou Vasileios\n",
    "3. Sykianakis Xaralambos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG1Bk3RH1Jnm"
   },
   "source": [
    "# Dataset Description\n",
    "\n",
    "\n",
    "\n",
    "*   Games File: Game features like game complexity, category etc\n",
    "*   User Ratings File: User ratings of board games\n",
    "*   Mechanics File: More detailed info on game characteristics\n",
    "*   Themes File: More detailed info on game characteristics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SpRkQbR_nw6"
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "\n",
    "!pip install alibi\n",
    "!pip install umap-learn\n",
    "!pip install optuna\n",
    "!pip install anchor-exp\n",
    "!pip install dice_ml\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "c67SVSui_CC_"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*colsample_bytree.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*subsample.*\")\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import umap\n",
    "import optuna\n",
    "from anchor import anchor_tabular\n",
    "from alibi.explainers import AnchorTabular\n",
    "import dice_ml\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlbZmJFuNhaw",
    "outputId": "d57665e7-e5eb-41be-84ce-e34b95b032ec"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qT9aVXy8X3ux"
   },
   "outputs": [],
   "source": [
    "#folder_path = Path('/content/drive/MyDrive/Colab Notebooks/Advanced Customer Analytics/Interpretable Predictions/Recommender_Files')\n",
    "folder_path = r'C:\\Users\\maniat\\OneDrive - Lyse AS\\Desktop\\personal\\MSc\\Customer Analytics\\Recommender Classification\\Recommender_Files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GVyH10HG_nw8"
   },
   "outputs": [],
   "source": [
    "def get_description(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "     Takes a dataframe as argument and collapses many\n",
    "    binary one-hot encoded column names to text\n",
    "    \"\"\"\n",
    "    cols = df.columns[15:]\n",
    "    categories = df[cols].gt(0).apply(lambda x: x.index[x].tolist(), axis=1)\n",
    "\n",
    "    return categories.apply(lambda x: ','.join(x) if x else 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zXCJODRz_nw8"
   },
   "outputs": [],
   "source": [
    "def downcasting_types(df: pd.DataFrame):\n",
    "  for column in df.columns:\n",
    "    colType = df[column].dtype\n",
    "    if colType == 'float64' :\n",
    "      df[column] = pd.to_numeric(df[column], downcast='float')\n",
    "    elif colType == 'int64' :\n",
    "      df[column] = pd.to_numeric(df[column], downcast='integer')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cK-NccX9_nw8"
   },
   "outputs": [],
   "source": [
    "def load_data(path: str):\n",
    "    \"\"\"\n",
    "    Load all CSV files in the path folder and return a dictionary of DataFrames.\n",
    "\n",
    "    \"\"\"\n",
    "    folder = Path(path)\n",
    "    if not folder.exists() or not folder.is_dir():\n",
    "        raise FileNotFoundError(f\"{path} directory was not found\")\n",
    "\n",
    "    dataframes = {}\n",
    "    for file in folder.glob('*.csv'):\n",
    "        file_name = file.stem\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes[file_name] = df\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ntf0zt8w_nw9"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_merge(dataframes: dict):\n",
    "    \"\"\"\n",
    "    Preprocess and merge DataFrames from the given dictionary\n",
    "    and returns a tuple ,(user_ratings, games_df).\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_keep = ['BGGId', 'GameWeight', 'MfgPlaytime', 'NumAlternates', 'NumExpansions',\n",
    "                               'NumImplementations', 'Kickstarted', 'Cat:Thematic', 'Cat:Strategy', 'Cat:War',\n",
    "                               'Cat:Family', 'Cat:CGS', 'Cat:Abstract', 'Cat:Party', 'Cat:Childrens']\n",
    "\n",
    "    for file_name, df in dataframes.items():\n",
    "        if file_name == \"games\":\n",
    "            # Preprocessing for 'games'\n",
    "            df = df[columns_to_keep].copy()\n",
    "            renaming_dict = {col: col.replace('Cat:', '') if col.startswith('Cat:')\n",
    "                             else col for col in df.columns}\n",
    "            df.rename(columns=renaming_dict, inplace=True)\n",
    "\n",
    "            # A small cleaning step\n",
    "            df = df[(df['MfgPlaytime'] > 0) & (df['MfgPlaytime'] <= 240) & (df['GameWeight'] > 0)]\n",
    "\n",
    "            # Discretizer\n",
    "            # discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "            # df[columns_to_discretize] = discretizer.fit_transform(df[columns_to_discretize])\n",
    "\n",
    "\n",
    "        elif file_name == \"themes\":\n",
    "            # Preprocessing for 'themes'\n",
    "            renaming_dict = {col: col.replace('Theme_', '') if col.startswith('Theme_')\n",
    "                             else col for col in df.columns}\n",
    "            df.rename(columns=renaming_dict, inplace=True)\n",
    "\n",
    "        elif file_name == \"mechanics\":\n",
    "            pass\n",
    "\n",
    "        elif file_name == \"user_ratings\":\n",
    "\n",
    "            df['Rating'] = df['Rating'].apply(lambda x: 1 if x >= 7 else -1)\n",
    "\n",
    "            label_encoder = LabelEncoder()\n",
    "            df['uid'] = label_encoder.fit_transform(df['Username'])\n",
    "            df.drop('Username', axis=1, inplace=True)\n",
    "\n",
    "            df = df.groupby('uid').filter(lambda x: len(x) >= 300).groupby('BGGId').filter(lambda x: len(x) >= 100)\n",
    "            df.drop_duplicates(subset=['uid', 'BGGId'], keep='last', inplace=True)\n",
    "\n",
    "        dataframes[file_name] = df\n",
    "\n",
    "    # Prepare final games dataframe\n",
    "    merged_df = pd.merge(dataframes['games'], dataframes['mechanics'], on='BGGId', how='left')\n",
    "    games_df = pd.merge(merged_df, dataframes['themes'], on='BGGId', how='left')\n",
    "    games_df['Details'] = get_description(games_df)\n",
    "\n",
    "    #columns_to_drop = dataframes['themes'].columns[1:].tolist() + dataframes['mechanics'].columns[1:].tolist()\n",
    "    columns_to_drop = [col for col in dataframes['themes'].columns.tolist() + dataframes['mechanics'].columns.tolist() if col != 'BGGId']\n",
    "\n",
    "    games_df = games_df.drop(columns=columns_to_drop, axis=1)\n",
    "    games_df = downcasting_types(games_df)\n",
    "\n",
    "    # Prepare final user_ratings dataframe\n",
    "    user_ratings_df = dataframes.get('user_ratings', pd.DataFrame())\n",
    "    # user_ratings_df = dataframes.get('user_ratings', None)\n",
    "    user_ratings_df = downcasting_types(user_ratings_df)\n",
    "\n",
    "\n",
    "    return user_ratings_df, games_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VWLzRkEX_nw9"
   },
   "outputs": [],
   "source": [
    "def create_user_clusters(train_df: pd.DataFrame,\n",
    "                         test_df: pd.DataFrame,\n",
    "                         uid_col: str, #user id col\n",
    "                         gameid_col: str, #game id col\n",
    "                         rating_col: str): #rating col\n",
    "    \"\"\"\n",
    "    Create user clusters for both training and test data\n",
    "    using UMAP for dimensionality reduction and DBSCAN for clustering\n",
    "    and returns a tuple of the clusters for the X_train and X_test\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    user_game_matrix_train = train_df.pivot(index=uid_col, columns=gameid_col, values=rating_col).fillna(0)\n",
    "\n",
    "    # UMAP to reduce dimensions on training data\n",
    "    reduction = umap.UMAP(n_neighbors=20, n_components=100, metric='cosine', min_dist=0.0, random_state=42)\n",
    "    embedding_train = reduction.fit_transform(user_game_matrix_train)\n",
    "\n",
    "    # DBSCAN on training data\n",
    "    dbscan = DBSCAN(eps=0.26, min_samples=15, metric='euclidean')\n",
    "    clusters_train = dbscan.fit_predict(embedding_train)\n",
    "    user_game_matrix_train['Cluster'] = clusters_train\n",
    "\n",
    "    # Transform test data using the trained UMAP model, Cluster with DBSCAN\n",
    "    user_game_matrix_test = test_df.pivot(index=uid_col, columns=gameid_col, values=rating_col).fillna(0)\n",
    "    embedding_test = reduction.transform(user_game_matrix_test)\n",
    "    clusters_test = dbscan.fit_predict(embedding_test)\n",
    "    user_game_matrix_test['Cluster'] = clusters_test\n",
    "\n",
    "    return user_game_matrix_train[['Cluster']].reset_index(), user_game_matrix_test[['Cluster']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "v1QVjxLR_nw9"
   },
   "outputs": [],
   "source": [
    "def prepare_train_test_split(path: str):\n",
    "    \"\"\"\n",
    "    Loads data, preprocesses it, merges, and returns\n",
    "    the train, test dataframes.\n",
    "    \"\"\"\n",
    "    # Call load_data, preprocess_and_merge\n",
    "    dataframes = load_data(path)\n",
    "    user_ratings_df, games_df = preprocess_and_merge(dataframes)\n",
    "\n",
    "    X = pd.merge(user_ratings_df, games_df, on='BGGId', how='left')\n",
    "    X = X.dropna()                          # CHECK HERE\n",
    "    y_model = X['Rating']\n",
    "    X_model = X.drop(['Details'], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_model, y_model, test_size=0.20)\n",
    "\n",
    "    train_for_clustering = X_train[['uid', 'BGGId', 'Rating']]\n",
    "    test_for_clustering = X_test[['uid', 'BGGId', 'Rating']]\n",
    "\n",
    "    user_clusters_train, user_clusters_test = create_user_clusters(train_for_clustering, test_for_clustering, 'uid', 'BGGId', 'Rating')\n",
    "\n",
    "    X_train = X_train.merge(user_clusters_train, on='uid', how='left')\n",
    "    X_test = X_test.merge(user_clusters_test, on='uid', how='left')\n",
    "\n",
    "    X_train = X_train.drop(['Rating'], axis=1)\n",
    "    X_train = downcasting_types(X_train)\n",
    "    X_test = X_test.drop(['Rating'], axis=1)\n",
    "    X_test = downcasting_types(X_test)\n",
    "    y_train = y_train.replace(-1, 0)\n",
    "    y_test = y_test.replace(-1, 0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RHcCqcBs_nw-"
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(X_train: pd.DataFrame,\n",
    "                             y_train: pd.DataFrame,\n",
    "                             n_trials: int = 5,\n",
    "                             cv_folds: int = 5):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter optimization using Optuna for LGBMClassifier.\n",
    "    Uses X_train and y_train for n_trials and returns the best\n",
    "    parameters of the optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.5),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 100)\n",
    "        }\n",
    "\n",
    "\n",
    "        model = LGBMClassifier(objective='binary',\n",
    "                               metric='binary_logloss',\n",
    "                               **params, n_jobs=-1)\n",
    "\n",
    "        # CV\n",
    "        scores = cross_val_score(model, X_train, y_train,\n",
    "                                 cv=cv_folds, scoring='roc_auc')\n",
    "        return scores.mean()\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    best_params = study.best_params\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kDtWUsdK_nw-"
   },
   "outputs": [],
   "source": [
    "def train_model_with_evaluation(X_train: pd.DataFrame,\n",
    "                                y_train: pd.DataFrame,\n",
    "                                X_test: pd.DataFrame,\n",
    "                                y_test: pd.DataFrame,\n",
    "                                n_trials: int, # if optuna = 1 , the number of trials\n",
    "                                n_estimators: int,  # number of boosting rounds for LGBM,\n",
    "                                optuna_on=0 #turn to 1 if you want to use\n",
    "                               ):\n",
    "    \"\"\"\n",
    "    Trains the model using the best parameters found\n",
    "    by Optuna and evaluates it on the test set and returns the\n",
    "    trained LightGBM model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    if optuna_on==1:\n",
    "      best_params = optimize_hyperparameters(X_train, y_train, n_trials=n_trials)\n",
    "      best_params[\"objective\"] = \"binary\"\n",
    "      best_params[\"metric\"] = \"binary_logloss\"\n",
    "      best_params[\"n_estimators\"] = n_estimators\n",
    "      best_params[\"n_jobs\"] = -1\n",
    "      model = LGBMClassifier(**best_params)\n",
    "    else:\n",
    "      model = LGBMClassifier()\n",
    "\n",
    "    early_stopping_callback = early_stopping(stopping_rounds=100, first_metric_only=True, verbose=True)\n",
    "    log_evaluation_callback = log_evaluation(period=50)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[early_stopping_callback, log_evaluation_callback])\n",
    "\n",
    "\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print(\"ROC AUC on the test set:\", roc_auc)\n",
    "\n",
    "    return model, roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SEEforf_nw-",
    "outputId": "e9d059d9-6c0a-4161-f512-f94b5c8b64ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_train_test_split(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"BGGId\", \"uid\"]\n",
    "\n",
    "X_train_unindexed = X_train.drop(columns=columns_to_drop, axis=1)\n",
    "X_test_unindexed = X_test.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qX-xmsHT_nw-",
    "outputId": "3c079990-d370-4c4c-f5c9-f5ea6d84a3e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:14:54,908] A new study created in memory with name: no-name-aa67dff1-d277-4913-b9fe-92cdba5c623a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.39393059814653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.39393059814653\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3542627176633044, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3542627176633044\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.39393059814653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.39393059814653\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3542627176633044, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3542627176633044\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.39393059814653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.39393059814653\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3542627176633044, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3542627176633044\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.39393059814653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.39393059814653\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3542627176633044, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3542627176633044\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:19:12,528] Trial 0 finished with value: 0.7201856451682647 and parameters: {'num_leaves': 137, 'learning_rate': 0.46816586404489124, 'feature_fraction': 0.39393059814653, 'bagging_fraction': 0.3542627176633044, 'bagging_freq': 3, 'min_child_samples': 26}. Best is trial 0 with value: 0.7201856451682647.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9542381410075251, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9542381410075251\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8275338717385091, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8275338717385091\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9542381410075251, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9542381410075251\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8275338717385091, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8275338717385091\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9542381410075251, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9542381410075251\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8275338717385091, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8275338717385091\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9542381410075251, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9542381410075251\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8275338717385091, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8275338717385091\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9542381410075251, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9542381410075251\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8275338717385091, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8275338717385091\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:25:27,685] Trial 1 finished with value: 0.6938320772166744 and parameters: {'num_leaves': 117, 'learning_rate': 0.022181670043159272, 'feature_fraction': 0.9542381410075251, 'bagging_fraction': 0.8275338717385091, 'bagging_freq': 2, 'min_child_samples': 6}. Best is trial 0 with value: 0.7201856451682647.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:29:03,717] Trial 2 finished with value: 0.7220707185821899 and parameters: {'num_leaves': 149, 'learning_rate': 0.28173904701437485, 'feature_fraction': 0.718456476789845, 'bagging_fraction': 0.32802529381527185, 'bagging_freq': 7, 'min_child_samples': 80}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.48377333676573053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48377333676573053\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11322240307576766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.11322240307576766\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48377333676573053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48377333676573053\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11322240307576766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.11322240307576766\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48377333676573053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48377333676573053\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11322240307576766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.11322240307576766\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48377333676573053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48377333676573053\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11322240307576766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.11322240307576766\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.48377333676573053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.48377333676573053\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.11322240307576766, subsample=1.0 will be ignored. Current value: bagging_fraction=0.11322240307576766\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:32:30,999] Trial 3 finished with value: 0.7192766952415324 and parameters: {'num_leaves': 219, 'learning_rate': 0.483141934627089, 'feature_fraction': 0.48377333676573053, 'bagging_fraction': 0.11322240307576766, 'bagging_freq': 7, 'min_child_samples': 54}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.13627713879452094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.13627713879452094\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5204372631725166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5204372631725166\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.13627713879452094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.13627713879452094\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5204372631725166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5204372631725166\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.13627713879452094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.13627713879452094\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5204372631725166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5204372631725166\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.13627713879452094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.13627713879452094\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5204372631725166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5204372631725166\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.13627713879452094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.13627713879452094\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5204372631725166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5204372631725166\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:35:44,690] Trial 4 finished with value: 0.6769085095409852 and parameters: {'num_leaves': 234, 'learning_rate': 0.08921616392553422, 'feature_fraction': 0.13627713879452094, 'bagging_fraction': 0.5204372631725166, 'bagging_freq': 6, 'min_child_samples': 100}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5503011944643811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5503011944643811\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.18512124770121843, subsample=1.0 will be ignored. Current value: bagging_fraction=0.18512124770121843\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5503011944643811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5503011944643811\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.18512124770121843, subsample=1.0 will be ignored. Current value: bagging_fraction=0.18512124770121843\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5503011944643811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5503011944643811\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.18512124770121843, subsample=1.0 will be ignored. Current value: bagging_fraction=0.18512124770121843\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5503011944643811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5503011944643811\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.18512124770121843, subsample=1.0 will be ignored. Current value: bagging_fraction=0.18512124770121843\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5503011944643811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5503011944643811\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.18512124770121843, subsample=1.0 will be ignored. Current value: bagging_fraction=0.18512124770121843\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:38:59,836] Trial 5 finished with value: 0.6756696019811754 and parameters: {'num_leaves': 47, 'learning_rate': 0.011570223903115583, 'feature_fraction': 0.5503011944643811, 'bagging_fraction': 0.18512124770121843, 'bagging_freq': 3, 'min_child_samples': 86}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5237832667063116, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5237832667063116\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4068975112290827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4068975112290827\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5237832667063116, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5237832667063116\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4068975112290827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4068975112290827\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5237832667063116, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5237832667063116\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4068975112290827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4068975112290827\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5237832667063116, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5237832667063116\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4068975112290827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4068975112290827\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5237832667063116, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5237832667063116\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4068975112290827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4068975112290827\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:41:36,050] Trial 6 finished with value: 0.6969608818554294 and parameters: {'num_leaves': 16, 'learning_rate': 0.44930358986658886, 'feature_fraction': 0.5237832667063116, 'bagging_fraction': 0.4068975112290827, 'bagging_freq': 10, 'min_child_samples': 29}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.20351572280375335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20351572280375335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7503966033714795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7503966033714795\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20351572280375335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20351572280375335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7503966033714795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7503966033714795\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20351572280375335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20351572280375335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7503966033714795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7503966033714795\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20351572280375335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20351572280375335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7503966033714795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7503966033714795\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n",
      "[LightGBM] [Warning] feature_fraction is set=0.20351572280375335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.20351572280375335\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7503966033714795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7503966033714795\n",
      "[LightGBM] [Warning] bagging_freq is set=10, subsample_freq=0 will be ignored. Current value: bagging_freq=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:46:08,377] Trial 7 finished with value: 0.6890212696569902 and parameters: {'num_leaves': 105, 'learning_rate': 0.07262753049396804, 'feature_fraction': 0.20351572280375335, 'bagging_fraction': 0.7503966033714795, 'bagging_freq': 10, 'min_child_samples': 52}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4935751493444046, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4935751493444046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5638249932739738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5638249932739738\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4935751493444046, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4935751493444046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5638249932739738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5638249932739738\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4935751493444046, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4935751493444046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5638249932739738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5638249932739738\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4935751493444046, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4935751493444046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5638249932739738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5638249932739738\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4935751493444046, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4935751493444046\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5638249932739738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5638249932739738\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:51:05,553] Trial 8 finished with value: 0.7120849477133699 and parameters: {'num_leaves': 153, 'learning_rate': 0.107090115659838, 'feature_fraction': 0.4935751493444046, 'bagging_fraction': 0.5638249932739738, 'bagging_freq': 3, 'min_child_samples': 73}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.22586566429952662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.22586566429952662\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7144541192563403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7144541192563403\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.22586566429952662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.22586566429952662\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7144541192563403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7144541192563403\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.22586566429952662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.22586566429952662\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7144541192563403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7144541192563403\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.22586566429952662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.22586566429952662\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7144541192563403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7144541192563403\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.22586566429952662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.22586566429952662\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7144541192563403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7144541192563403\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-21 07:53:14,230] Trial 9 finished with value: 0.6910162487817175 and parameters: {'num_leaves': 64, 'learning_rate': 0.13027564723281562, 'feature_fraction': 0.22586566429952662, 'bagging_fraction': 0.7144541192563403, 'bagging_freq': 5, 'min_child_samples': 28}. Best is trial 2 with value: 0.7220707185821899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.718456476789845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.718456476789845\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.32802529381527185, subsample=1.0 will be ignored. Current value: bagging_fraction=0.32802529381527185\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.591309\n",
      "[100]\tvalid_0's binary_logloss: 0.587268\n",
      "[150]\tvalid_0's binary_logloss: 0.585663\n",
      "[200]\tvalid_0's binary_logloss: 0.58583\n",
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's binary_logloss: 0.585541\n",
      "Evaluated only: binary_logloss\n",
      "ROC AUC on the test set: 0.7385691801998124\n"
     ]
    }
   ],
   "source": [
    "trained_model,roc_auc = train_model_with_evaluation(X_train_unindexed, y_train, X_test_unindexed, y_test, n_trials=10, n_estimators=1000,optuna_on=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(trained_model: lgb.LGBMModel,\n",
    "                       X_train: pd.DataFrame,\n",
    "                       X_test: pd.DataFrame,\n",
    "                       y_train: pd.Series,\n",
    "                       categorical_features: List[str]):\n",
    "    \"\"\"\n",
    "    Function to explain a prediction made by a trained LGBM binary classifier model.\n",
    "    Generates an Anchor explanation for a random pair or user,game\n",
    "    If the prediction is 0, it also generates a counterfactual explanation using DiCE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = pd.concat([X_train, X_test])\n",
    "    new_user = dataset.drop_duplicates(subset='uid').loc[:, ['uid', 'Cluster']]\n",
    "    new_game = dataset.drop_duplicates(subset='BGGId').drop(columns=['uid', 'Cluster'])\n",
    "\n",
    "    # Pick random user and game\n",
    "    random_user = new_user['uid'].sample(n=1).iloc[0]\n",
    "    random_game = new_game['BGGId'].sample(n=1).iloc[0]\n",
    "\n",
    "    # Filter dataframes for the user and game pair\n",
    "    selected_user = new_user[new_user['uid'] == random_user]\n",
    "    selected_game = new_game[new_game['BGGId'] == random_game]\n",
    "    test_instance = pd.concat([selected_user.reset_index(drop=True), selected_game.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    \n",
    "    test_instance = test_instance[X_train.columns].drop(columns=['uid', 'BGGId'])\n",
    "    print(\"Created Test Instance\\n\\n\",test_instance)\n",
    "    prediction = trained_model.predict(test_instance)[0]\n",
    "    \n",
    "    print('Anchor Explanation')\n",
    "    # ANCHOR EXPLANATION\n",
    "    columns_to_drop = [\"BGGId\", \"uid\"]\n",
    "\n",
    "    X_train_unindexed = X_train.drop(columns=columns_to_drop, axis=1)\n",
    "    X_test_unindexed = X_test.drop(columns=columns_to_drop, axis=1)\n",
    "    categorical_names = {}\n",
    "    headers = list(X_train_unindexed)\n",
    "    for idx, feature in enumerate(headers):\n",
    "        if feature in categorical_features:\n",
    "          categorical_names[idx] = list(X_train_unindexed[feature].unique())\n",
    "        \n",
    "    explainer = AnchorTabular(\n",
    "    trained_model.predict,\n",
    "    feature_names=X_train_unindexed.columns.tolist(),\n",
    "    categorical_names=categorical_names)\n",
    "\n",
    "    explainer.fit(X_train_unindexed.to_numpy())\n",
    "    \n",
    "    # Generate Anchor explanation\n",
    "    result = explainer.explain(np.array(test_instance.values))\n",
    "    \n",
    "    #result = explainer.explain(test_instance.values)\n",
    "    \n",
    "    print('Prediction:', trained_model.predict(np.array(test_instance.values).reshape(1,-1))[0])\n",
    "    print('Anchor:', result.data['anchor'])\n",
    "    print(\"Precision: {:.2f}\".format(result.data['precision']))\n",
    "    print(\"Coverage: {:.2f}\".format(result.data['coverage']))\n",
    "    \n",
    "#     print('\\n\\nASHAP Tree explainer')\n",
    "    \n",
    "#     # SHAP Tree explainer\n",
    "#     explainer = shap.TreeExplainer(trained_model)\n",
    "#     shap_instance_values = explainer.shap_values(test_instance)\n",
    "#     shap.force_plot(explainer.expected_value, shap_instance_values, instance_to_explain)  \n",
    "        \n",
    "    \n",
    "    # Counterfactuals with DiCE (if prediction is 0)\n",
    "    if prediction == 0:\n",
    "        print('\\n\\nACounterfactuals with DiCE')\n",
    "        # Create the dataset for DiCE\n",
    "        X_train_labeled = X_train_unindexed.reset_index(drop=True)\n",
    "        y_train_labeled = y_train.reset_index(drop=True)\n",
    "\n",
    "        data_new = pd.concat([X_train_labeled,y_train_labeled], axis=1)\n",
    "\n",
    "        d = dice_ml.Data(\n",
    "            dataframe=data_new, \n",
    "            continuous_features=[\n",
    "                'GameWeight', 'MfgPlaytime', 'NumAlternates', 'NumExpansions', 'NumImplementations'], \n",
    "            outcome_name='Rating'\n",
    "        )\n",
    "\n",
    "        backend = 'sklearn'\n",
    "        m = dice_ml.Model(model=trained_model, backend=backend)\n",
    "\n",
    "        exp = dice_ml.Dice(d, m)\n",
    "\n",
    "        dice_exp = exp.generate_counterfactuals(test_instance,\n",
    "                                                total_CFs=4,\n",
    "                                                desired_class=\"opposite\",\n",
    "                                                features_to_vary=['GameWeight', 'MfgPlaytime', 'NumAlternates', 'NumExpansions', 'NumImplementations'])\n",
    "\n",
    "        # Visualize the counterfactuals\n",
    "        dice_exp.visualize_as_dataframe(show_only_changes=True)\n",
    "        \n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Test Instance\n",
      "\n",
      "    GameWeight  MfgPlaytime  NumAlternates  NumExpansions  NumImplementations  \\\n",
      "0      1.4685         45.0            4.0            0.0                 1.0   \n",
      "\n",
      "   Kickstarted  Thematic  Strategy  War  Family  CGS  Abstract  Party  \\\n",
      "0          0.0       0.0       0.0  0.0     1.0  0.0       0.0    0.0   \n",
      "\n",
      "   Childrens  Cluster  \n",
      "0        0.0        0  \n",
      "Anchor Explanation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'numpy.float32' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKickstarted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThematic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrategy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFamily\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCGS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParty\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChildrens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexplain_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 51\u001b[0m, in \u001b[0;36mexplain_prediction\u001b[1;34m(trained_model, X_train, X_test, y_train, categorical_features)\u001b[0m\n\u001b[0;32m     48\u001b[0m explainer\u001b[38;5;241m.\u001b[39mfit(X_train_unindexed\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Generate Anchor explanation\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#result = explainer.explain(test_instance.values)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction:\u001b[39m\u001b[38;5;124m'\u001b[39m, trained_model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(test_instance\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py:855\u001b[0m, in \u001b[0;36mAnchorTabular.explain\u001b[1;34m(self, X, threshold, delta, tau, batch_size, coverage_samples, beam_size, stop_on_first, max_anchor_size, min_samples_start, n_covered_ex, binary_cache_size, cache_margin, verbose, verbose_every, **kwargs)\u001b[0m\n\u001b[0;32m    842\u001b[0m result: Any \u001b[38;5;241m=\u001b[39m mab\u001b[38;5;241m.\u001b[39manchor_beam(\n\u001b[0;32m    843\u001b[0m     delta\u001b[38;5;241m=\u001b[39mdelta, epsilon\u001b[38;5;241m=\u001b[39mtau,\n\u001b[0;32m    844\u001b[0m     desired_confidence\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     verbose_every\u001b[38;5;241m=\u001b[39mverbose_every,\n\u001b[0;32m    852\u001b[0m )\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmab \u001b[38;5;241m=\u001b[39m mab\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_explanation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py:878\u001b[0m, in \u001b[0;36mAnchorTabular._build_explanation\u001b[1;34m(self, X, result, predicted_label, params)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_explanation\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: np\u001b[38;5;241m.\u001b[39mndarray, result: \u001b[38;5;28mdict\u001b[39m, predicted_label: \u001b[38;5;28mint\u001b[39m, params: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Explanation:\n\u001b[0;32m    858\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m    Preprocess search output and return an explanation object containing metdata\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    875\u001b[0m \n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_names_to_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([predicted_label])\n\u001b[0;32m    880\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ord_to_ohe(np\u001b[38;5;241m.\u001b[39matleast_2d(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_vars_ord)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\alibi\\explainers\\anchors\\anchor_tabular.py:938\u001b[0m, in \u001b[0;36mAnchorTabular.add_names_to_exp\u001b[1;34m(self, explanation)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feat_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_values:\n\u001b[0;32m    937\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(v)\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    939\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_values[feat_id][v]):\n\u001b[0;32m    940\u001b[0m         fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    941\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (fname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_values[feat_id][v])\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'numpy.float32' is not iterable"
     ]
    }
   ],
   "source": [
    "categorical_features = ['Kickstarted', 'Thematic', 'Strategy', 'War', 'Family', 'CGS', 'Abstract', 'Party', 'Childrens', 'Cluster']\n",
    "explain_prediction(trained_model, X_train, X_test, y_train,categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WXc_0De8nDIA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
